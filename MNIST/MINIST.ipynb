{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras Tutorial: The Ultimate Beginnerâ€™s Guide to Deep Learning in Python "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Step 1: Import libraries and modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing numpy and setting a seed for the computer's pseudorandom number generator. This allows us to reproduce the results from our script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(123)  # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll import the Sequential model type from Keras. This is simply a linear stack of neural network layers, and it's perfect for the type of feed-forward CNN we're building in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's import the \"core\" layers from Keras. These are the layers that are used in almost any neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Activation, Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll import the CNN layers from Keras. These are the convolutional layers that will help us efficiently train on image data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Convolution2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll import some utilities. This will help us transform our data later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything we need to build our neural network architecture."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Step 2: Load image data from MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST is a great dataset for getting started with deep learning and computer vision. It's a big enough challenge to warrant neural networks, but it's manageable on a single computer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Keras library conveniently includes it already. We can load it like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    " \n",
    "# Load pre-shuffled MNIST data into train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "#We can look at the shape of the dataset:\n",
    "\n",
    "print(X_train.shape)\n",
    "# (60000, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so it appears that we have 60,000 samples in our training set, and the images are 28 pixels x 28 pixels each. We can confirm this by plotting the first sample in matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xb2d4b1f60>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, when working with computer vision, it's helpful to visually plot the data before doing any algorithm work. It's a quick sanity check that can prevent easily avoidable mistakes (such as misinterpreting the data dimensions)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Step 3: Preprocess input data for Keras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must explicitly declare a dimension for the depth of the input image. For example, a full-color image with all 3 RGB channels will have a depth of 3.\n",
    "\n",
    "Our MNIST images only have a depth of 1, but we must explicitly declare that.\n",
    "\n",
    "In other words, we want to transform our dataset from having shape (n, width, height) to (n, depth, width, height).\n",
    "\n",
    "Here's how we can do that easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm, we can print X_train's dimensions again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "# (60000, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final preprocessing step for the input data is to convert our data type to float32 and normalize our data values to the range [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our input data are ready for model training."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Step 4: Preprocess class labels for Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's take a look at the shape of our class label data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "# (60000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm... that may be problematic. We should have 10 different classes, one for each digit, but it looks like we only have a 1-dimensional array. Let's take a look at the labels for the first 10 training samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9 2 1 3 1 4]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:10])\n",
    "# [5 0 4 1 9 2 1 3 1 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there's the problem. The y_train and y_test data are not split into 10 distinct class labels, but rather are represented as a single array with the class values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fix this easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "#Now we can take another look:\n",
    "print(Y_train.shape)\n",
    "# (60000, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go... much better!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Step 5:  Define model architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to define our model architecture. In actual work, researchers will spend a considerable amount of time studying model architectures.\n",
    "\n",
    "To keep this tutorial moving along, we're not going to discuss the theory or math here. This alone is a rich and meaty field, and we recommend the CS231n (http://cs231n.github.io/) class mentioned earlier for those who want to learn more.\n",
    "\n",
    "Plus, when you're just starting out, you can just replicate proven architectures from academic papers or use existing examples. Here's a list of example implementations in Keras. https://github.com/fchollet/keras/tree/master/examples\n",
    "\n",
    "Let's start by declaring a sequential model format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we declare the input layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Convolution2D(32, (3, 3), activation='relu', input_shape=(1,28,28), data_format='channels_first'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input shape parameter should be the shape of 1 sample. In this case, it's the same (1, 28, 28) that corresponds to  the (depth, width, height) of each digit image.\n",
    "\n",
    "But what do the first 3 parameters represent? They correspond to the number of convolution filters to use, the number of rows in each convolution kernel, and the number of columns in each convolution kernel, respectively.\n",
    "\n",
    "*Note: The step size is (1,1) by default, and it can be tuned using the 'subsample' parameter.\n",
    "\n",
    "We can confirm this by printing the shape of the current model output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 32, 26, 26)\n"
     ]
    }
   ],
   "source": [
    "print(model.output_shape)\n",
    "# (None, 32, 26, 26)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can simply add more layers to our model like we're building legos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/py3/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "model.add(Convolution2D(32, 3, 3, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we won't go into the theory too much, but it's important to highlight the Dropout layer we just added. This is a method for regularizing our model in order to prevent overfitting. You can read more about it here https://www.quora.com/How-does-the-dropout-method-work-in-deep-learning.\n",
    "\n",
    "MaxPooling2D is a way to reduce the number of parameters in our model by sliding a 2x2 pooling filter across the previous layer and taking the max of the 4 values in the 2x2 filter.\n",
    "\n",
    "So far, for model parameters, we've added two Convolution layers. To complete our model architecture, let's add a fully connected layer and then the output layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Dense layers, the first parameter is the output size of the layer. Keras automatically handles the connections between layers.\n",
    "\n",
    "Note that the final layer has an output size of 10, corresponding to the 10 classes of digits.\n",
    "\n",
    "Also note that the weights from the Convolution layers must be flattened (made 1-dimensional) before passing them to the fully connected Dense layer.\n",
    "\n",
    "Here's how the entire model architecture looks together:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all we need to do is define the loss function and the optimizer, and then we'll be ready to train it."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Step 6: Compile model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're in the home stretch! The hard part is already over.\n",
    "\n",
    "We just need to compile the model and we'll be ready to train it. When we compile the model, we declare the loss function and the optimizer (SGD, Adam, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Keras has a variety of loss functions (https://keras.io/objectives/) and out-of-the-box optimizers (https://keras.io/optimizers/) to choose from."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Step 7: Fit model on training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fit the model, all we have to do is declare the batch size and number of epochs to train for, then pass in our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 152s 3ms/step - loss: 0.8643 - acc: 0.7118\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 147s 2ms/step - loss: 0.3780 - acc: 0.8869\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 146s 2ms/step - loss: 0.2980 - acc: 0.9100\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 144s 2ms/step - loss: 0.2583 - acc: 0.9224\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 143s 2ms/step - loss: 0.2340 - acc: 0.9309\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 150s 2ms/step - loss: 0.2087 - acc: 0.9380\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 147s 2ms/step - loss: 0.2004 - acc: 0.9404\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 150s 2ms/step - loss: 0.1905 - acc: 0.9435\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 152s 3ms/step - loss: 0.1788 - acc: 0.9468\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 161s 3ms/step - loss: 0.1678 - acc: 0.9500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb44555438>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, \n",
    "          batch_size=32, epochs=10, verbose=1)\n",
    "# Epoch 1/10\n",
    "# 7744/60000 [==>...........................] - ETA: 96s - loss: 0.5806 - acc: 0.8164"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy, huh?\n",
    "\n",
    "You can also use a variety of callbacks to set early-stopping rules, save model weights along the way, or log the history of each training epoch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Step 8: Evaluate model on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can evaluate our model on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0835387240434764, 0.9738]\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations... you've made it to the end of this Keras tutorial!\n",
    "\n",
    "We've just completed a whirlwind tour of Keras's core functionality, but we've only really scratched the surface. Hopefully you've gained the foundation to further explore all that Keras has to offer.\n",
    "\n",
    "For continued learning, we recommend studying other example models in Keras (https://github.com/fchollet/keras/tree/master/examples) and Stanford's computer vision class (http://cs231n.stanford.edu/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The complete code, from start to finish.\n",
    "Here's all the code in one place, in a single script."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#1 import libraries and modules\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    " \n",
    "# 2. Load pre-shuffled MNIST data into train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    " \n",
    "# 3. Preprocess input data\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    " \n",
    "# 4. Preprocess class labels\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)\n",
    " \n",
    "# 5. Define model architecture\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(32, (3, 3), activation='relu', input_shape=(1,28,28), data_format='channels_first'))\n",
    "model.add(Convolution2D(32, 3, 3, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    " \n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    " \n",
    "# 6. Compile model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "# 7. Fit model on training data\n",
    "model.fit(X_train, Y_train, \n",
    "          batch_size=32, epochs=10, verbose=1)\n",
    " \n",
    "# 8. Evaluate model on test data\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
